```plants
library(ggplot2)
library(tidyverse)
```plants
library(ggplot2)
library(tidyverse)
```{r setup, echo=FALSE}
library(ggplot2)
library(tidyverse)
```{r setup, echo=FALSE}
library(tidyverse)
library(ggplot2)
library(ggplot2)
library(tidyverse)
```{r setup, echo=FALSE}
plants <- read_excel("~/Downloads/Bio Lab Data.xlsx")
view(plants)
#The salinity and nitrogen levels are numbers.
##We need to tell R that these are factors so it recognizes the treatments as groups.
plants$`Nitrogen Level` <- as.factor(plants$`Nitrogen Level`)
plants$Genotype <- as.factor(plants$Genotype)
Above_Ground_Biomass_(g)_summary <- plants %>%
Above Ground Biomass (g) summary <- plants %>%
Above Ground Biomass (g) summary <- plants %>%
Above.Ground.Biomass.(g).summary <- plants %>%
Above_Ground_Biomass_(g)_summary <- plants %>%
AGB_summary <- plants %>%
group_by(Nitrogen_Level, Genotype) %>%
summarize(mean = mean(Above_Ground_Biomass_(g)),
sd = sd(Above_Ground_Biomass_(g))
sd = sd(Above_Ground_Biomass_(g))
AGB_summary <- plants %>%
group_by(Nitrogen_Level, Genotype) %>%
summarize(mean = mean(Above_Ground_Biomass_(g))
sd = sd(Above_Ground_Biomass_(g))
AGB_summary <- plants %>%
group_by(Nitrogen_Level, Genotype) %>%+ summarize(mean = mean(Above_Ground_Biomass_(g))
+ sd = sd(Above_Ground_Biomass_(g))
AGB_summary <- plants %>%
group_by(Nitrogen_Level, Genotype) %>%
summarize(mean = mean(Above_Ground_Biomass_(g))
sd = sd(Above_Ground_Biomass_(g))
AGB_summary <- plants %>%
group_by(Nitrogen_Level, Genotype) %>%
summarize(mean = mean(Above_Ground_Biomass_(g))
sd = sd(Above_Ground_Biomass_(g))
AGB_summary <- plants %>%
group_by(Nitrogen_Level, Genotype) %>%
summarize(mean = mean(Above_Ground_Biomass_(g))
sd=sd(Above_Ground_Biomass_(g))
AGB_summary <- plants %>%
group_by(Nitrogen_Level, Genotype) %>%
summarize(mean = mean(Above_Ground_Biomass_(g))
sd = sd(Above_Ground_Biomass_(g))
Above_Ground_Biomass_(g)_summary
AGB_summary <- plants %>%
group_by(Nitrogen_Level, Genotype) %>%
summarize(mean = mean(Above_Ground_Biomass_(g))
sd = sd(Above_Ground_Biomass_(g))
AGB_summary <- plants %>%
group_by(Nitrogen_Level, Genotype) %>%
summarize(mean = mean(Above_Ground_Biomass_(g)),
library(readxl)
plant <- read_excel("Downloads/plant.xlsx")
View(plant)
sd = sd(Above_Ground_Biomass_(g))
library(ggplot2)
library(tidyverse)
plants$`Nitrogen` <- as.factor(plants$`Nitrogen`)
plants$Nitrogen <- as.factor(plants$Nitrogen)
plants$Nitrogen <- as.factor(plants$Nitrogen)
library(readxl)
plant <- read_excel("Downloads/plant.xlsx")
View(plant)
plants <- read.csv('fakedata.csv')
plants <- read_excel("Downloads/plant.xlsx")
# define a vector x. x contains 2, 4, 6, and 8.
x <- c(2,4,6,8)
# can also use = operator
# create vector from 1 to 10
x <- c(1:10)
#or
view(baseball)
#or
View(baseball)
# define a vector x. x contains 2, 4, 6, and 8.
x <- c(2,4,6,8)
# can also use = operator
# compute the mean and st dev of x
mean(x)
sd(x)
# return the 4th element of x
x[4]
# return the 3rd and 4th element of x
x[3:4]
# read from files
# set directory
setwd("C:/Users/laurenlatimer/Stat-Methods")
# check directory
getwd()
# use read.table() to read data of .txt format
baseball <- read.table("baseball2011.txt", header=T)
# data is a csv file
?read.csv
# option 2: instead of setting wd, just define full path
baseball <- read.table("C:/Users/laurenlatimer/Stat-Methods/basebal2011.txt", header=T)
# look at data by calling variable
baseball
#or
View(baseball)
# option 2: instead of setting wd, just define full path
baseball <- read.table("C:/Users/laurenlatimer/Stat-Methods/basebal2011.txt", header=T)
# option 2: instead of setting wd, just define full path
baseball <- read.table("C:/Users/laurenlatimer/Stat-Methods/basebal2011.txt", header=T)
# look at data by calling variable
baseball
# option 2: instead of setting wd, just define full path
baseball <- read.table("C:/Users/laurenlatimer/Stat-Methods/basebal2011.txt", header=T)
# define a vector x. x contains 2, 4, 6, and 8.
x <- c(2,4,6,8)
# compute the mean and st dev of x
mean(x)
sd(x)
# read from files
# set directory
setwd("C:/Users/laurenlatimer/Stat-Methods")
# return the 4th element of x
x[4]
# check directory
getwd()
# option 2: instead of setting wd, just define full path
baseball <- read.table("C:/Users/laurenlatimer/Stat-Methods/basebal2011.txt", header=T)
source("~/Stat-Methods/Lab 0.R", echo=TRUE)
# Check the current working directory.
current_dir <- getwd()
# Set the working directory.
setwd("C:/Users/laurenlatimer/Stat-Methods")
# Set the working directory.
setwd("C:/Users/laurenlatimer/Downloads")
# Set the working directory.
setwd("C:/Users/laurenlatimer")
# Use read.table() to read data from a .txt file (assuming "baseball2011.txt" is correct).
baseball <- read.table("baseball2011.txt", header = TRUE)
# Use read.table() to read data from a .txt file (assuming "baseball2011.txt" is correct).
baseball <- read.table("baseball2011.txt", header = TRUE)
# Look at the data by printing the variable.
print(baseball)
# Or use View() to open a data viewer.
View(baseball)
# Or use View() to open a data viewer.
view(baseball)
Team
View(Team)
View("Team")
# This is equivalent to baseball[, 1].
baseball[4]
baseball[,4]
1-pnorm(99.7, 98.6, 0.4)
pnorm(99.2,98.6,0.4) - pnorm(98.1, 98.6, 0.4)
qnorm(0.35, 98.6, 0.4)
sd(c(98.3, 98.4, 99.0, 98.9))
1 - pnorm(98.4, 98.6, 0.4/sqrt(12))
0.4/sqrt(12)
install.packages("glmbb")
library(glmbb)
data(crabs)
hist(crabs$width, main= "Histogram of Carapace Widths", xlab= "Widths(cm)")
hist(crabs$width, main= "Histogram of Carapace Widths", xlab= "Widths(cm)", breaks=20)
cram_mean
mean(crabs)
crab_mean - qnorm(0.95, 0, 1)*crab_sd/sqrt(173)
crabs_mean
crabs_mean = mean(crabs)
knitr::opts_chunk$set(echo = TRUE)
hist(crabs$width, main = "Histogram of Carapace Widths", xlab = "Widths (cm)")
crab_mean
# lowerbound of 90% confidence interval
crab_mean - qnorm(.95,0,1)*crab_sd/sqrt(173)
View(crabs)
crab_mean = mean(crabs$width)
crab_mean = mean(crabs$width)
crab_sd = sd(crabs$width)
# lowerbound of 90% confidence interval
crab_mean - qnorm(.95,0,1)*crab_sd/sqrt(173)
crab_mean + qnorm(.95,0,1)*crab_sd/sqrt(173)
qt(0.95,df=173-1)
?t.test
t.test(crabs$width, alternative = 'less', mu = 28, conf.level = 0.99)
getwd()
Auto <- read.csv("Auto.csv")
Auto <- read.csv("Auto.csv")
hist(Auto$Cost, main = "Hudson Auto Repair Parts Cost", xlab = "Cost ($)", breaks = 5)
mean(Auto$Cost)
std(Auto$Cost)
sd(Auto$Cost)
t.test(Auto$Cost, alternative = 'greater', mu = 75.5, conf.level = 0.95)
Use the t score
qt(0.9, 9)
-------
Use the t score
qt(0.05, 9)
-------
pnorm
pnorm(105,100,16)
knitr::opts_chunk$set(echo = TRUE)
install.packages('faraway')
library(faraway)
install.packages("faraway")
install.packages("faraway")
install.packages("faraway")
install.packages("faraway")
Yes
knitr::opts_chunk$set(echo = TRUE)
install.packages('faraway')
library(faraway)
install.packages("faraway")
# fit the lm model
?lm()
punt.regress <- lm(punting$Distance ~ punting$RFlex)
punt.regress
# fit the lm model
?lm()
punt.regress <- lm(punting$Distance ~ punting$RFlex)
install.packages('faraway')
library(faraway)
summary(punt.regress)
beta1_hat<-2.6871
beta1_se<-0.5943
t<-qt(0.05,11)
beta1_hat - t*beta1_se
beta1_hat + t*beta1_se
summary(punt.regress)
r_square<-0.6502
r <- sqrt(r_square)
install.packages('faraway')
library(faraway)
install.packages('faraway')
library(faraway)
install.packages('lme4')
library(faraway)
install.packages('faraway')
library(faraway)
?plot
plot(punting$RFlex, punting$Distance, xlab="", ylab="Distance", main="Title")
punt.regress <- lm(punting$Distance ~ punting$RFlex)
# fit the lm model
?lm()
punt.regress <- lm(punting$Distance ~ punting$RFlex)
# create the scatterplot
plot(punting$RFlex, punting$Distance, xlab = "Right Hamstring Flexibility (10 punts)")
# add the regression line
abline(punt.regress)
qnorm(0.05, 1.13, 2.21)
qnorm(0.005, 1.13, 2.21)
qt(0.025, 5)
y <-c(0.95, 0.83, 1.2, 0.89, 1.45, 1.12)
mean(y)
var(y)
x <-c(25,23,21,21,20)
mean(x)
var(x)
pt(7.826, 4)
qt(0.995, 4)
x[1:3,]
x[3]
x[1:4]
x[,1:4]
# part 2
# load iris.csv and do the following:
setwd('/Users/laurenlatimer/Biocomputing/Biocomputing-Ex-8')
iris_data <- read.csv("iris.csv", header=TRUE)
# part 2
# load iris.csv and do the following:
setwd('/Users/laurenlatimer/Biocomputing/Biocomputing-Ex-8')
iris_data <- read.csv("iris.csv", header=TRUE)
View(iris_data)
# c. get rows with Sepal.Width > 3.5
sepal_data <- which(iris_data$Sepal.Width > 3.5)
# b. get the number of observations for each species included in the data set
setosa_data <- which(iris_data$Species == 'setosa')
length(setosa_data)
setosa_count <- length(setosa_data)
versicolor_data <- which(iris_data$Species == 'versicolor')
versicolor_count <- length(versicolor_data)
virginica_data <- which(iris_data$Species == 'virginica')
virginica_count <- length(virginica_data)
# b. get the number of observations for each species included in the data set
table(iris_data$Species)
help("write.csv")
# d. write the data for the species setosa to a comma-delimited file names 'setosa.csv'
write.csv(setosa_data, file="setosa.csv", quote=FALSE, sep=",")
setosa_data
# d. write the data for the species setosa to a comma-delimited file names 'setosa.csv'
setosa_data <- iris_data[setosa_rows, ]
# part 2
# load iris.csv and do the following:
setwd('/Users/laurenlatimer/Biocomputing/Biocomputing-Ex-8')
iris_data <- read.csv("iris.csv", header=TRUE)
# b. get the number of observations for each species included in the data set
table(iris_data$Species)
setosa_rows <- which(iris_data$Species == 'setosa')
setosa_count <- length(setosa_rows)
versicolor_rows <- which(iris_data$Species == 'versicolor')
versicolor_count <- length(versicolor_rows)
virginica_rows <- which(iris_data$Species == 'virginica')
virginica_count <- length(virginica_rows)
# c. get rows with Sepal.Width > 3.5
sepal_data <- which(iris_data$Sepal.Width > 3.5)
# d. write the data for the species setosa to a comma-delimited file names 'setosa.csv'
setosa_data <- iris_data[setosa_rows, ]
write.csv(setosa_data, file="setosa.csv", quote=FALSE)
# e. calculate the mean, minimum, and maximum Petal.Length for observations from virginica
virginica_data <- iris_data[virginica_rows, ]
avg_plength <- mean(virginica_data$Petal.Length)
min_plength <- min(virginica_data$Petal.Length)
max_plength <- min(virginica_data$Petal.Length)
max_plength <- max(virginica_data$Petal.Length)
# a. print the last 2 rows in the last 2 columns to the R terminal
print(iris_data[149:150, 4:5])
knitr::opts_chunk$set(echo = TRUE)
library(faraway)
data(troutegg)
help(troutegg)
trout.regress <- glm(survive/total ~ location+ period, data = troutegg, family = "binomial", weights = total)
summary(trout.regress)
pchisq(956.97, 7, lower.tail = FALSE)
trout.regress <- glm(survive/total ~ location+ period + location:period, data = troutegg, family = "binomial", weights = total)
summary(trout.regress)
library(faraway)
data(hsb)
unique(hsb$prog)
install.packages('nnet')
library(nnet)
program.regress <- multinom(prog~ses+math, data = hsb)
program.regress
predictors <- data.frame(74, 'high')
colnames(predictors) <- c("math", "ses")
View(predictors)
# use the predict function to predict the probability for all categories
predict(program.regress,newdata = predictors, type='probs')
program.regress
library(generalhoslem)
help(ghl.test)
install.packages(generalhoslem)
install.packages(generalhoslem)
install.packages(generalhoslem)
install.packages("generalhoslem")
install.packages("generalhoslem")
library(generalhoslem)
ghl.test <- logitgof(hsb$prog, program.regress$fitted.values, g = 5, ord=F)
ghl.test
ghl.test$observed
ghl.test$expected
ghl.test$observed
ghl.test$expected
# or
setosa_rows <- iris_data[iris_data$Species == 'setosa', ]
# part 2
# load iris.csv and do the following:
setwd('/Users/laurenlatimer/Biocomputing/Biocomputing-Ex-8')
iris_data <- read.csv("iris.csv", header=TRUE)
# a. print the last 2 rows in the last 2 columns to the R terminal
print(iris_data[149:150, 4:5])
# b. get the number of observations for each species included in the data set
table(iris_data$Species)
# or
setosa_rows <- iris_data[iris_data$Species == 'setosa', ]
View(setosa_rows)
# or
setosa_data <- iris_data[iris_data$Species == 'setosa', ]
setosa_count <- length(setosa_data)
versicolor_data <- iris_data[iris_data$Species == 'versicolor', ]
versicolor_count <- length(versicolor_data)
setosa_count <- nrow(setosa_data)
versicolor_data <- iris_data[iris_data$Species == 'versicolor', ]
versicolor_count <- nrow(versicolor_data)
virginica_data <- iris_data[iris_data$Species == 'virginica', ]
virginica_count <- nrow(virginica_data)
# c. get rows with Sepal.Width > 3.5
sepal_data <- iris_data[iris_data$Sepal.Width > 3.5, ]
View(sepal_data)
# d. write the data for the species setosa to a comma-delimited file names 'setosa.csv'
write.csv(setosa_data, file="setosa.csv", quote=FALSE)
# e. calculate the mean, minimum, and maximum Petal.Length for observations from virginica
avg_plength <- mean(virginica_data$Petal.Length)
min_plength <- min(virginica_data$Petal.Length)
max_plength <- max(virginica_data$Petal.Length)
# part 1
# replicate functionality of head -
file_path <- "birthdays.txt"  # replace with desired file path
num_lines <- 2  # replace with any number of desired lines
# read the file
file_content <- readLines(file_path)
# create subset of selected lines
selected_lines <- file_content[1:num_lines]
print(selected_lines, sep = "\n")
num_lines <- 4  # replace with any number of desired lines
print(selected_lines, sep = "\n")
# create subset of selected lines
selected_lines <- file_content[1:num_lines]
print(selected_lines, sep = "\n")
print(selected_lines)
# create subset of selected lines
selected_lines <- file_content[1:num_lines]
print(selected_lines)
num_lines <- 10  # replace with any number of desired lines
# create subset of selected lines
selected_lines <- file_content[1:num_lines]
print(selected_lines)
# part 2
# load iris.csv and do the following:
setwd('/Users/laurenlatimer/Biocomputing/Biocomputing-Ex-8')
iris_data <- read.csv("iris.csv", header=TRUE)
# a. print the last 2 rows in the last 2 columns to the R terminal
print(iris_data[149:150, 4:5])
# b. get the number of observations for each species included in the data set
table(iris_data$Species)
# or
setosa_data <- iris_data[iris_data$Species == 'setosa', ]
setosa_count <- nrow(setosa_data)
versicolor_data <- iris_data[iris_data$Species == 'versicolor', ]
versicolor_count <- nrow(versicolor_data)
virginica_data <- iris_data[iris_data$Species == 'virginica', ]
virginica_count <- nrow(virginica_data)
# c. get rows with Sepal.Width > 3.5
sepal_data <- iris_data[iris_data$Sepal.Width > 3.5, ]
# d. write the data for the species setosa to a comma-delimited file names 'setosa.csv'
write.csv(setosa_data, file="setosa.csv", quote=FALSE)
# e. calculate the mean, minimum, and maximum Petal.Length for observations from virginica
avg_plength <- mean(virginica_data$Petal.Length)
min_plength <- min(virginica_data$Petal.Length)
max_plength <- max(virginica_data$Petal.Length)
read.csv("setosa.csv", header=TRUE)
View(iris_data)
setosa_file <- read.csv("setosa.csv", header=TRUE)
View(setosa_file)
# d. write the data for the species setosa to a comma-delimited file names 'setosa.csv'
write.csv(setosa_data, file="setosa.csv", quote=FALSE, row.names=FALSE)
setosa_file <- read.csv("setosa.csv", header=TRUE)
View(setosa_file)
setwd('~/Stat-Methods/final-project')
tuition <- read.csv("tuition_data.csv", header=TRUE)
View(tuition)
library(ggplot2)
ggplot(data=tuition, aes(x=in_state_total, y=mid_career_pay)) + geom_point()
ggplot(data=tuition, aes(x=in_state_total, y=mid_career_pay, fill=type)) + geom_point()
ggplot(data=tuition, aes(x=in_state_total, y=mid_career_pay, fill=type)) + geom_point() + theme_classic()
ggplot(data=tuition, aes(x=in_state_total, y=mid_career_pay)) + geom_point()
plot_a <- ggplot(data=tuition, aes(x=in_state_total, y=mid_career_pay)) + geom_point()
plot_a + stat_smooth(method=lm)
plot_a + stat_smooth(method=gam)
ggplot(data=tuition, aes(x=in_state_total, y=mid_career_pay)) + geom_point() + geom_smooth(method=lm)
ggplot(data=tuition, aes(x=in_state_total, y=mid_career_pay)) + geom_point() + geom_smooth()
help("geom_smooth")
ggplot(data=tuition, aes(x=in_state_tuition, y=mid_career_pay)) + geom_point() + geom_smooth()
ggplot(data=tuition, aes(x=in_state_tuition, y=mid_career_pay)) + geom_point() + geom_smooth(method=lm)
ggplot(data=tuition, aes(x=in_state_total, y=mid_career_pay)) + geom_point() + geom_smooth(method=lm)
tuition$net_cost <- as.numeric(tuition$net_cost)
ggplot(data=tuition, aes(x=net_cost, y=mid_career_pay)) + geom_point() + geom_smooth(method=lm)
ggplot(data=tuition, aes(x=in_state_total, y=mid_career_pay)) + geom_point() + geom_smooth(method=lm)
# part 2
setwd('/Users/laurenlatimer/Biocomputing/Biocomputing_Exercise9')
data <- read.table("data.txt", header=TRUE)
View(data)
# a. barplot of the means of the four populations
ggplot(data, aes(x=region)) +
geom_bar() +
theme_classic() +
xlab("Region")
View(data)
# a. barplot of the means of the four populations
ggplot(data, aes(x=region, y=mean(observations))) +
geom_bar() +
theme_classic() +
xlab("Region")
# a. barplot of the means of the four populations
ggplot(data, aes(x=region, y=mean(X..observations))) +
geom_bar() +
theme_classic() +
xlab("Region")
print(data)
mean(X..observations.)
mean(data[,2])
data <- read.table("data.txt", sep="," header=TRUE)
View(data)
xlab("Region")
# a. barplot of the means of the four populations
ggplot(data, aes(x=region)) +
geom_bar() +
theme_classic() +
xlab("Region")
# part 2
setwd('/Users/laurenlatimer/Biocomputing/Biocomputing_Exercise9')
data <- read.table("data.txt", sep="," header=TRUE)
data <- read.table("data.txt", sep=",", header=TRUE)
View(data)
# a. barplot of the means of the four populations
ggplot(data, aes(x=region, y=observations)) +
geom_bar() +
theme_classic() +
xlab("Region")
# a. barplot of the means of the four populations
ggplot(data, aes(x=region)) +
geom_bar() +
theme_classic() +
xlab("Region")
rev_data <-  table(data$region)
print(rev_data)
north_data <- data[data$region == 'north', ]
east_data <- data[data$region == 'east', ]
south_data <- data[data$region == 'south', ]
west_data <- data[data$region == 'west', ]
